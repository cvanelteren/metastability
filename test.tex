% Created 2022-04-06 Wed 15:55
% Intended LaTeX compiler: pdflatex
\documentclass[a4paper, 11pt, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{cleveref}
\newcommand{\m}[1]{\textbf{#1}}
\newcommand{\infdecay}{I(s_i : S^t)}
\usepackage{biblatex}
\addbibresource{library.bib}


\usepackage[english]{babel}
\author{Casper van Elteren}
\date{\today}
\title{Analysis of bottom-up generation of noise-induced transitions in networks}
\begin{document}

\maketitle
% om mijn gedachten te ordenen maak ik vaak zelf een abstract. Niet als teken dat jouw abstract niet goed zou zijn maar dus als exercitie. Hier is ie. Ik denk wel dat deze dichterbij is wat nature communications wil zien: breed toegankelijk en niet teveel technische termen. Jouw abstract heeft ook wat inaccuracies, we hebben het bijv helemaal niet over criticality (voor fysici betekent dit bijv fasetransities, diverging correlation length, etc), en we hebben het over noise-induced transitions specifiek. Kun je een nieuwe abstract maken uit deze twee? Voel je vrij om te gebruiken wat je wilt van de mijne. Ik heb wel elements van jou gebruikt trouwens. Dit is basically mijn counterproposal.
\lettrineabstract{
Abrupt, system-wide transitions can be endogenously generated by seemingly stable networks of interacting dynamical units, such as mode switching in neuronal networks or public opinion changes in social systems. However, it remains poorly understood how such 'noise-induced transitions' are generated by the interplay of network structure and dynamics on the network. Here we use information theory to discover the progression of the fluctuations through the network which eventually lead to a long-term systemic transition. We identify two key roles for nodes in this progression. In the initial phase, certain dynamic nodes pass on short-lived fluctuations to neighboring nodes, making them temporarily more dynamic in turn. Conversely, towards the tipping point we identify other nodes whose state information becomes part of the long-term memory of the system. For the example of node dynamics governed by a Boltzman-Gibbs distribution we find that the onset of a tipping point tends to percolate from low-degree to high-degree nodes along paths where nodes gradually grow in connectivity. We also show that identifying the different roles enables performing different types of targeted interventions that make tipping points more (less) likely to begin or to lead to systemic change. In general this progression depends on the combination of network structure and dynamics, which can be discovered using our methodology. This opens up possibilities for understanding and controlling endogenously generated metastable behavior.
}

% \lettrineabstract{Noise  plays a  fundamental  role in  many
% real-world   systems   from    neural   behavior,   cellular
% regulation,  physics   and  social   systems.  It   is  well
% established that even weak noise can result in large changes
% in  system behavior  such as  transitions between  or escape
% quasi-stable  states. These  transitions  can correspond  to
% critical events such as catastrophic failures or extinctions
% that make  them essential to understand  and quantify. Here,
% we propose two mechanism fundamental to metastable behavior.
% First,  short-lived correlations  are essential  to initiate
% metastable transitions. Second,  long-lived correlations are
% essential to  stabilize after  a metastable  transition. Two
% new  measures are  introduced that  can quantify  the system
% stability  and the  initiator for  the mestable  transitions
% using tools from information theory. Metastable behavior was
% studied  in  a  general  a  general  class  of  systems  are
% considered governed by  the Boltzmann-Gibss distribution. In
% particular,     the     kinetic      Ising     model     and
% Susceptible-Infected-Susceptible model are used. By means of
% measuring   information   flows,    the   contributions   of
% short-lived and  long-term correlations are  observed before
% and during the metastable  transition. The results show that
% short-lived correlations  are essential to absorb  noise and
% transmit the  noise through  the network,  whereas long-term
% correlations are  essential to retain stability  outside the
% metastable transition.  The results suggest that  the choice
% of  control  variables  for metastable  transitions  differs
% remarkable  depending  on  the   stability  of  the  system.
% Information flows  offers a  unique way to  study metastable
% transitions  and may  provide novel  sensitivity markers  to
% control  and  prevent   metastable  behavior  in  real-world
% complex systems.}


\section{Introduction}
\label{sec:orgd6a1d62}
Complex dynamical systems such as cell-regulatory processes,
epidemic  spreading   dynamics,  opinion   formation  neural
behavior and so on,  often exhibit extended periods of robust systemic behavior.
By correlating fluctuations, networks can endogenously induce a
rapid transition from one stable systemic
behavior to  another. These so-called noise-induced events  are often
rare events  and may be associated  with either desirable or undesirable outcome
such  as the onset or extinction of  an infectious  disease or the onset of epileptic seizure.
% hier komt kanker voorbij maar dat kwam niet al voorbij in zin 1, ik zou dat congruent houden dus ofwel zin 1 aanpassen ofwel (mijn voorkeur) dit voorbeeld veranderen (mode changes in neuronal networks? Specifiek voorbeeld?).
The  mechanisms
underlying these transitions are  often not well understood.
It  is  of vital  importance  to  understand the  underlying
processes  that cause  metastable behavior  to quantify  the
impact of noise on complex systems.

% ik zou deze paragraaf verplaatsen naar wat verderop, en de hourglass model wat smooth houden (dit is namelijk al best concreet en de volgende paragraaf zoomt weer wat uit)
Here, we  consider dynamical systems consisting of a static network where the states of the nodes are governed by a Boltzmann-Gibbs
distribution. This type of model has been used to describe a wide range of
behaviors such as  neural dynamics \cite{cite_needed}, opinion
dynamics,  ferromagnetic  spins \cite{Glauber1963}, and  organized
criminal  gang  interactions  \cite{DOrsogna2015a}.
%and  so  on \cite{cite_needed}.
In this class of models, each node chooses its state in equilibrium with the potential induced by its neighboring states. In physical applications this potential is the classic energy potential, but in other applications it can be interpreted, for instance as frustration level, homophily, or more broadly speaking, a fitness score of the node state given its neighbors. The second ingredient in this model is a global `temperature' which is essentially a noise level: at zero noise a node always picks the absolute minimum energy state, whereas the higher this noise level, the more likely it is that high energy states are chosen.

% RQ: deze paragraaf was half overlappend met de paragraaf hierboven. De paragraaf hierboven moet dus het model zelf uitleggen. Deze paragraaf kan dan focusen op het metastabiele aspect alleen, dat op zich vind ik een goed idee. Dus ik schrijf hieronder iets, wat in feite een omdraaiing is van jouw paragraaf en dan ook wat weggelaten.
% From a statistical physics  perspective, the system dynamics
% can  be  understood  as  a  particle  moving  on  an  energy
% landscape    (fig.    \ref{fig:introduction}).   The    system
% configuration for  this ``system particle'' is  represented by
% the current system state. Over time the system particle will
% traverse the  energy landscape, evaluating its  next move by
% evaluating the energy of the system at a new position in the
% energy  landscape.  New  system states  will  preferable  be
% chosen of new  locations with lower energy  than the current
% state. In this  view metastability can be  understood as the
% system  particle  moving  between  local  energy  minima  by
% ``climbing''  out of  these  minima and  crossing a  so-called
% ``tipping  point''  (the  high-point  of  the  energy  barrier
% separating metastable states).
These models lead generally to metastable behavior because there are usually multiple (local) minima in the system's potential. In finite systems and non-zero temperature, there is a finite probability that the system moves (eventually) from one local minimum to another. Without loss of generality, in this paper we illustrate our method using the well-known kinetic Ising spin model without external forces. Here, nodes have only two possible states: +0 and +1. At system level there are two global minima: all nodes in state +0 or all nodes in state +1. Between these two system states lies a `potential barrier': many possible paths of system states which connect the two systemic minima, but all of which have a growing potential, making these paths less likely than paths of similar length that remain close to one of the minima. The peak of this potential along each crossing path lies informally speaking at a `checkerboard pattern': each node being maximally different from (the majority of) its neighbors. We refer to this peak as the `tipping point'. See \cref{fig:introduction}.

% RQ: ik heb dit weggehaald uit de paragraaf hierboven omdat het over iets heel anders begint, namelijk het doel van de studie en daarna over de statistische aanpak, terwijl de paragraaf begint met welk model we gebruiken en hoe breed toepasbaar dat wel of niet is.
%The  goal  of this  study  the  essential
%mechanism   underlying   metastable    behavior   by   using
%statistical tools  from information  theory. It  is proposed
%that short-lived  correlations are essential  to destabilize
%the system  which allows long-term correlations  to move the
%system state to another absorbing state.

% wat wil je precies zeggen met deze paragraaf? Is deze distinction belangrijk voor de lezer? Take home message? Indien geen dan zou ik hem weglaten.
%Noise-induced transitions can be  understood to be caused by
%either    external     or    internal    \cite{Forgoston2018}.
%Traditionally,  external noise  is studied  by replacing  an
%external control variable with a stochastic process, whereas
%internal  noise  is caused  by  random  interactions of  the
%elements  of the  system.  For example  the transmission  of
%COVID-19 from  one individual  to another  random individual
%through  homogeneous mixing.  For both  types of  noise, the
%study of  metastability results in finding  the optimal path
%between metastable points either numerically or by analysis.
%The metastable trajectories are the analyzed in terms of the
%mean  exit times  or mean  transition times,  but yields  no
%insights \emph{how}  the metastable transition occured.  That is,
%what set of  steps are essential for  metastable behavior to
%occur.



% A  tipping point  is a  point in  the energy  landscape that
% separates  to metastable  points (fig.  \ref{fig:introduction}
% red circle). For deterministic  dynamics the trajectory that
% the system  particle makes  is completely determined  by the
% initial  state and  the system  dynamics. Consequently,  the
% system particle will always move towards to lowest reachable
% point  given  the  current  position  of  the  particle.  In
% contrast,  with  stochastic  (noisy)  system  dynamics,  the
% system is able to escape local minima.


% RQ: ik denk dat ik het hier wel eens bent met wat je wilt zeggen, maar de takehome message komt niet heel scherp over. Je begint over zeldzaamheid van transitions, dan een praktische techniek om toch statistiek te kunnen bedrijven (optimal paths), dan wat fysica, en daarna iets over het aanwezig zijn of niet van een externe parameter, en als laatste nog eens iets over zelf-heling van dieren.
% Ik begin de paragraaf hieronder met wat volgens mij de cruciale takehome message is.
% Understanding the impact of noise induced transitions on the
% system dynamics  of complex systems are  often difficult due
% to  the rarity  of transitions.  Consequently, analysis  and
% computation often focus on  the most important noise-induced
% events such as coexisting  stable states, critical failures,
% and nucleation of coherent structures. These critical events
% are often studied  by considering the optimal  paths for the
% system  to transition  into  the metastable  state. Less  is
% known, however, how the system  \emph{uses} the noise to traverse
% these  paths \cite{McDonnell2009}.  For  example,  it is  well
% known that metastable states  can be traversed in stochastic
% systems   by   controlling    a   noise   parameters   (fig.
% \ref{fig:introduction}).  The increase  in  noise changes  the
% energy landscape such that transitioning from one metastable
% state becomes energetically feasible. However, this requires
% access to  an external control variable.  The identification
% of this control-variable for  real-world complex systems may
% be difficult  identify or  completely lacking.  For example,
% some  organisms such  as the  sponges, or  octopi and  other
% amphibians are able to  self-organize damage to their bodies
% without   any  external   control   parameter.  The   system
% endogeneous   dynamics  drive   the   transition  from   one
% metastable state  to the  next.
The crucial point here is that the network structure can make systemic transitions much more likely than without it. Without network effects, each node has an independent probability of choosing the (unlikely) high potential state. The probability that all nodes in the system happen to do this simultaneously (thereby transitioning the system state to another potential minimum) decreases to zero rapidly (as $\mathcal{O}(e^{-N^2})$ for dense networks). This means that transitions become unlikely for all but the smallest systems. With network effects, however, transitions can potentially occur along a path of nodes that form a domino effect. That is, the first node choosing randomly a high potential state makes the same state transition more likely for all of its neighbors. For some of these neighbors this new situation may suffice to make the same transition with (almost) equal likelihood as the first node, and so on, until the tipping point has been reached. The likelihood of such a transition is much higher than without network effects (up to $\mathcal{O}(e^{-N})$). This is still an exponentially decaying function of system size, highlighting the fact that such noise-induced transitions still only occur in finite-size systems, but exceedingly more likely.

%RQ: prima paragraaf, maar ik denk dat het kan worden incorporated in die daarna.
% More importantly, traditional forms of analysis focus on the
% system as a  whole. Less is known how  the internal elements
% of the system  use the noise to iniate and  traverse the the
% energy landscape. That is, is there a constructive mechanism
% that  causes  the system  to  cross  between two  metastable
% states. For many complex systems, there exists an inherently
% diversity in the elements that make up the system. Making it
% essential to  design methods and metrics  that gain insights
% \emph{how} the  system uses noise to  traverse between metastable
% states.

Here we present a  method   to    uncover the network percolation process that facilitates  endogenous,   noise-induced
transitions. The computational method only requires access to cross-sections of time-series of observations of the  system, meaning
that it is broadly applicable.
% RQ: herhaling
% The   system  considered   follow   are   dictated  by   the
% Boltzmann-Gibbs  distribution that  has been  shown to  be a
% valid descriptor for many different kinds of systems ranging
% from neural  behavior, to the interaction  of gas particles,
% opinion  dynamics,  epidemic  spreading  and  so  on.

% The method consists of computing two features of the information flows in a system: decay time of information (short-term) and asymptotic information level (long-term).. [TODO] % RQ: probeer eens verder te schrijven (je paragraaf hieronder vervangend/anders verwoordend)? ik laat de rest van de paragraaf nu ongemoeid.

%CvE: Ik twijfel nog over goede benamingen voor de twee features. Ik dacht iets om eerst aan te halen dat we naar short en long-term dynamics kijken en vervolgens alleen te benoemen in grote lijnen hoe de maten heten (adjusted integrated mutual information en iets van rest information).
The method consists of analyzing two key features using information flows of a system: the time of the short-term information decay, and the long-term information level. Here, the contribution of each node to the system dynamics over time are considered. The results highlight that short-lived correlations measured by Shannon mutual information shared between and node and the entire system ($I(s_i : S^t$) are essential to absorb and transfer noise through the system. After the majority of the system crosses the tipping point, a new local equilibrium is established. These long-term correlations are essential for the system to maintain its metastable state.
The approach differs from traditional approaches that focus on how the system as a whole approaches a tipping point. Here, the mechanism underlying \emph{how} local connectivity of nodes contribute to the system dynamics can be understood and analyzed.
% Exact
% information flows are computed  as function of the stability
% of the system (see \ref{sec:org8193119}) and compared with
% the  structural features  of the  network. This  exposes the
% mechanism \emph{how} the system  uses noise to transition between
% metastable    points.   The    results    show   that    for
% out-of-equilibrium  dynamics  short-lived  correlations  are
% necessary for tipping behavior  to absorb and transfer noise
% through the  system. In contrast, long-term  correlations are
% necessary  for  maintaining the  system  state  in the  next
% (metastable) state. Information  theoretic measures are used
% to track the information flows as a function of the distance
% the  system is  to metastable  transition. The  results show
% that  as a  function of  the  stability of  the system,  the
% contribution to the information processing differs depending
% on the processes of  short-lived and long-term correlations.
% This may  form a  new way of  thinking for  preventing rapid
% state transitions.

\begin{figure*}
\centering
\includegraphics[width=.9\linewidth]{./figures/figure1.png}
\caption{\label{fig:introduction}(a) A system consists of elements (circles) with a coupled interaction structure (edges). Each node has some intrinsic dynamics indicated by the energy lines (gray); a low configurational energy corresponds with a ``stable'' state. Metstability are often considered on a system level (b), where a macrscopic system state is decomposed by some control parameter. Here, the system consists of two metastable states. The system in a current metastable state (green) can only transition into another stable state by crossing the tipping point (red). Out of equilibrium the temporal dynamics of such a transition are depicted in (c). Over time the temporal dynamics cause may cause metastable transitions (red dot c). By studying the information flows as a function of tipping distance (e), The integrated mutual information represents the area under the curve for the information decay of a node with the system over time ($I(s_i : S^t)$): it is a measure of how much the current node state, predicts the future system state. Asymptotic information forms an approximation of long time scale dynamics. In contrast, the integrated mutual information captures the short time scale dynamics out-of-equilibrium. Through information features, the mechanism underlying metastable transitions can be understood (d): far away from the tipping point, information processing occurs in low degree nodes, as the system approaches the tipping point, the higher degree nodes are recruited. The information cascade unravels the mechanisms whereby short-lived correlations are essential for priming the system for the metastable transition. For more information on numerical approaches see \ref{sec:org8193119}.}
\end{figure*}

\section{Results}
\label{sec:org6f0b033}

% leg de opzet van de studie uit
\Cref{fig:introduction} a-c shows a graphical representation of how a network of binary distribution governed by \cref{eq:glauber,eq:energy} can lead to bistability of the system macrostate. Some configurations of the system are more stable than other configurations (\cref{fig:introduction}b). As the system evolves over time, the system favors macrostates with higher stability.

In \cref{fig:introduction}c an example system trajectory is shown. Three markers indicate distinct states the system may be in. The green markers indicates and example where the system far away from the tipping point. The macrostate of the system is close to the stable state with all the nodes are in the +0 state. As the system evolves, local transient perturbations will destabilize the system macrostate. Most destabilization relax back to the meteastable point. For some noise-induced perturbations, however, the system macrostate rapidly transitions between metastable points (for example the green to blue to red circle transition).

The contribution of each node to the macrostate is equal, however the node contribution is not equal in terms for the macrostate reaching the tipping point. Consider for example a node with low degree, e.g. degree 1, and a node with high degree, e.g. degree 10 (\cref{fig:maj_flip}). A node with lower degree is more likely to flip given the state of its neighbors than a node with higher degree. Consequently, it is more likely that nodes with lower degree destabilize the system, pushing the system closer to the tipping point.

% Naar discussion
% By considering the system macrostate under the influence of two major forces, the mechanism underlying the tipping behavior is starting to show a remarkable structure. We distinguish destabilizer nodes as those nodes that are more likely to flip from the minority to the majority state. These nodes absorb and propagate noise into the system. Opposed are stabilizer nodes that prevent noise from being propagated through the system. These nodes are less likely to change their state under the influence of small perturbations.

From an information perspective, the contribution of the dynamics of a node can be quantified using time-delayed Shannon information. Depending on the connectivity of a node in the system, the contribution to the system macrostate will differ \cite{vanElteren2022,Quax2013}. How much the future system state is affected by the node's current state is computed by shared information with the node's current state $s_i$ and the future system state $S^t$ as the adjusted mutual integrated information

\begin{equation}
\label{eq:adj_imi}
\bar \mu_(s_i) = \sum_{t = 0}^\infty (I(s_i : S^t) - \omega_{s_i}) \Delta t.
\end{equation}

Intuitively $\mu(s_i)$ represents the transient dynamics of how much the influence of a node is "remembered" by the system over time. It reflects how the effects of local dynamics between nodes percolates through the system over time. As the system chooses it next metastable state, the system macrostate is dominated by transient dynamics. The next tipping point will be reached on a much longer time-scale. Consequently, $\omega$ quantifies the system returning to a stable system regime. For nodes with fast dynamics, $\mu(s_i)$ is generally high and $\omega_{s_i}$ would be generally low

In \cref{fig:introduction}d-f the information flows are computed that quantifies the contribution of a node on the future system state. The information flows are computed by considering the system in different states of stability (see \ref{appendix ref}). That is, the subset of states are selected such that states $S_{\gamma} = \{S' \subseteq S | M(S) = \gamma\}$ where $\gamma$ is the fraction of nodes having state +1. By evolving all possible trajectories, the exact information flows are computed for $t=500$ steps. Asymptotic and integrated mutual information are estimated using regression (\ref{appendix ref}).

Three things are observed. First, far away from the tipping point (\cref{fig:kite_res}a, nodes with lower degree are more dynamics yielding higher $\mu(s_i)$. In the stable regime, the nodes with lower degree that tend to destabilize the system flipping from majority state to the minority state. As the system approaches the tipping point, the transient dynamics of higher degree nodes increases. Second, the asymptotic information remains low far away from the tipping point, and becomes increasingly high as the system approaches the tipping point (\cref{fig:introduction}e,f and \cref{fig:kite_res}b). The increase of the asymptotic information reflects the likelihood of the system escaping the local metastable state and transitioning to another metastable state. For example, the green hub node in \cref{fig:kite_res} has maximum asymptotic information at the tipping point. After choosing the next state, the hub node can be used as a proxy to determine on which side of the tipping point remains.


The tipping point is reached by a sequence of flips starting from the nodes with the highest potential to flip. In \cref{fig:max_trajectory} a trajectory is shown that maximizes $\log P(S^t|S^{t-1}, M(S^5) = 0.5)$. The system starts in the stable regime with all nodes having state +0. The tipping point is reached by the lowest degree node flipping first, which promotes the probability of its neighbor flipping.








% Second,
% The transition between metastable states for metastable systems are generally rare. The instantaneous system macrostate is determined both by noise induced changes and a path dependency introduced by past states. These transient fluctuations
% \cref{fig:kite_res} shows a graphical representation of how a network of binary variables, each governed by \cref{eq:glauber,eq:energy}, can lead to bistability of the system state. It also shows how a transition between these system states is associated with a percolation of information flows through the network. % RQ: hierna zou ik in deze paragraaf wat in proza uitleggen wat de lezer ziet in het figuur, ook uitleggen dat het een voorbeeld is. Misschien kan je proberen jouw twee paragrafen onder "The results show two major features" kunnen samenvoegen en hier zetten. Eventueel in een aparte paragraaf twee, maar kan ook in deze eerste. Ik trok hier inspiratie uit de paragraaf Figure 1a..." uit https://www.nature.com/articles/ncomms10850 (wel al een beetje oud).

% The transient contribution of a node $\mu(s_i)$ quantifies how much the short-term fluctuations of a node are "remembered" by the short-term fluctuations of the system over time.  % RQ: verdere uitleg over hoe de lezer mu moet interpreteren. Misschien als kortetermijn memory van het systeem van fluctuaties van een node?
% RQ: belangrijk: in het eerste paragraaf, of eerste twee, waar je het figuur 1 uitlegt, zou ik nog algemene bewoordingen houden, zoals nodes met kortetermijnseffecten en langetermijnseffecten. In deze paragrafen ga je dan uitleggen hoe je dat gekwantificeerd hebt, en pas praten over informatie of memory e.d. Formule 1 kan er mooi bij en dan kun je beide symbolen benoemen.

% RQ: derde paragraaf: zo'n zelfde paragraaf voor het langetermijnsgeheugen van het systeem


% In \cref{fig:kite_res} the information flows over time are computed as function of the stability of the system. The contribution of a node to the system dynamics was quantified using time-delayed Shannon  mutual information  $I(s_i : S^t)$ between a node $s_i$ and the future system state $S^t$ (see \ref{sec:org3d3e541})
% \begin{equation}
% \label{eq:adj_imi}
% \bar \mu_(s_i) = \sum_{t = 0}^\infty (I(s_i : S^t) - \omega) \Delta t,
% \end{equation}
% where $\omega$ approximates the asymptotic information or the "rest" information that is left after the system chooses its next state (\cref{fig:introduction}). Intuitively, $\mu(s_i)$ represents the transient dynamics of how much the influence of a node is "remembered" by the system over time.

% The results show two major features. First, the information processing differs depending on the stability of the system (\cref{fig:kite_res}); far away from the tipping point most information processing occurs in low degree nodes. In these stable regimes, higher degree nodes are more difficult to escape and flip to the minority state (\cref{fig:maj_flip} and \ref{sec:org3d3e541}). As the system approaches the tipping point, higher and higher degrees contribute more to the transient dynamics as the nearest neighbors promote flipping to the majority state.

% Second, as the system approaches the tipping point a long-term rest information emerges. This asymptotic information represents the long-term behavior of the system as it chooses its next metastable state. That is, as the system is destabilized, the system may either relax to the prior metastable state or choose another metastable state. The transient dynamics reflect the information processing as the system chooses its next metastable state. The information decay significantly slows down as the system is destabilized, yielding an apparent asymptote that decays on a timescale much longer than the fast dynamics.

% The difference of information flows for different nodes implies distinct roles essential for the tipping behavior.  Two distinct roles can be identified; destabilizer nodes and stabilizer nodes. Destabilizers are nodes that are essential when the system is far away from the tipping point. These nodes destabilize the system macrostate due to their higher degrees of freedom. That is, these nodes are less constraint by neighboring nodes allowing them to more often switch states. This lowers the transition probability of neighboring nodes to flip, inducing a chain reaction for other nodes to flips and the macrostate to transition between metastable points.

% In order to test this hypothesis, external interventions were used to pin the state of a node \cref{fig:kite_noise}. Nodes were pinned to +0 state, which biases the system macrostate to have fewer nodes in the +1 state.  Destabilizing nodes absorb and inject noise into the system. \cref{fig:kite_noise} shows how intervention on destabilizing nodes (e.g. node 9 in \cref{fig:kite_noise,fig:kite_res}) removes white noise from the system trajectories (\cref{fig:kite_noise} c) when the system approaches the metastable point from all +0 side (\cref{fig:kite_noise} a), but increases the white noise for the system macrostate above 0.5 (\cref{fig:kite_noise} b). Notable, the number tipping point from +0 to +1 is lower than for +1 to +0. In contrast, interventions on high degree nodes yields increase levels of white noise for the +0 macrostate, but lower levels of white noise for the +1 macrostate. This effect is explained by  the stabilizing effect of higher degree nodes (\cref{fig:kite_noise} a, b, c). The system is pushed to the tipping point, but is less likely to traverse the tipping point. If the system does transition from +0 to +1 macrostate, the system is nudged back to the +0 state as the hub nodes are essential for remaining stability.  Combined the results imply that destabilizing nodes tend to be nodes with lower degree that are essential to produce white noise that leads to a chain of destabilization from lower to higher degree nodes. The high degree nodes are essential to finally tip the system when the local neighborhood of the high degree nodes are energetically favorable of flipping.

% The bottom of propagation of noise can be most readily seen in \cref{fig:max_trajectory} where a typical trajectory is plotted that maximizes the shortest path from the stable state in the same state to the tipping point without any interventions, i.e. it maximizes $\sum_{t=1}^5 p(S^t | S^{t-1}, M(S^5) = 0.5)$. The first node to flip is the node with the lowest degree. The noise is propagated to a node with degree 2 which flips and in turns "recruits" the degree 3 nodes. Each flips makes it more likely for the neighboring node to flip in addition to the minority state. As the noise-induced cascade continues higher and higher degree nodes are "recruited" and flipped.


% \begin{figure*}
% \centering
% \includegraphics[width=.9\linewidth]{./figures/ising_kite_graph.pdf}
% \caption{\label{fig:kite_res}(a) As the system approaches the tipping point the information processing moves from lower degree  nodes to higher degree nodes. Each node is governed by kinetic Ising dynamics. The node size is proportional to the adjusted integrated mutual information. (b) Information flows as a function of system stability. Far from the tipping point the information processing is mainly in lower degree nodes. As the system approaches the tipping point, the information flows increases for all nodes. Higher degree nodes tend to have higher adjusted integrated mutual information and higher information offset. The information offset encodes the long-time scale correlation of the node with the system state. A higher asymptotic information implies that the system remembers the node state for longer than other nodes.}
% \end{figure*}


% \begin{figure*}
% \centering
% \includegraphics[width=.9\linewidth]{./figures/kite_maximized_trajectory_30230.png}
% \caption{\label{fig:max_trajectory}The tipping point is initiated from the bottom up. Each node is colored according to state 0 (black) and state 1 (yellow) Shown is a trajectory towards the the tipping point that maximizes $\sum_{{t=1}}^{{5}} \log p(S^{{t-1}} | S^t, M(S^5) = 0.5)$. As the system approaches the tipping point, low degree nodes flip first, and recruite ``higher'' degree nodes to further destablize the system and push it towards a tipping point. There are in total 30240 trajectories that reach the tipping point in 5 steps, and there are 10 trajectories that have the same maximized values as the trajectory shown in this figure.}
% \end{figure*}


% Finally, we note that on  the tipping point, the information
% decays  for all  nodes more  similar than  further from  the
% tipping    point   (\cref{fig:kite_res}).   The    most
% distinguishing  feature is  the asymptotic  information. The
% similarity  in information  decay  can be  explained by  the
% shared  similarity in  the node  dynamics. That  is, at  the
% tipping point each node has  the same 50/50 distribution. In
% contrast, further  away from  the tipping point,  nodes with
% lower degree are  generate higher levels of  entropy. As the
% system relaxes  back into  a metastable point,  the relative
% impact that  each node  has on its  neighbors are  the same.
% That  is,  each node  ``moves''  the  neighbor's energy  by  a
% similar amount. The resulting  node-node dynamics result due
% to this similarity in a similar decay curve and consequently
% similar  adjusted integrated  mutual information.  The exact
% rates will  diverge over  time as  the immediate  degree and
% higher  order   network  structure   dissipates  information
% differently.



% \begin{figure*}
% \centering
% \includegraphics[width=.9\linewidth]{./figures/kite_pinning_summary.png}
% \caption{\label{fig:kite_noise}White noise of the system macrostate outside the tipping point. Numerical simulations were performed using 6 different seeds. (a, b) White noise was estimated for the instantaneous system macrostate for the two stable point (a, b) (see \ref{sec:orgc093508}). The intervention pinned the node at state +0. This causes the system to prefer the macrostate where the fractions of nodes are < 0.5 regardless of the node intervened on. Importantly, the figure shows that intervention on the lower degree nodes (e.g. 9 or 8) removes high frequency noise (c). Compared to the control condition (blue bands) the interventions on higher degree nodes (e.g. 4) produces more white noise for the system macrostate but less frequent tipping points.  The high frequency noise is essential to initiate the metastable transition whereas higher degree nodes are essential to retain the stability when the tipping occured. Interventions on higher degree nodes prevents the tipping point from occurring as the higher degree nodes have to flip as the system crosses the tipping point. Interventions on higher degree nodes therefore produce higher levels of white noise for (a) but less for (b) as the system macrostate does not make the metastable state that often. (c) Shown are a system trajectory for the krackhardt kite graph with seed 1234. An intervention pins the node state at state +0. The figure shows that intervention on lower degree nodes remove high frequency noise (e.g. see node 9 or 8) when the system macrostate is below 0.5, but increased when the system is above 0.5. For lower degree nodes the system is more stable when the macrostate is below 0.5. In contrast, interventions on higher degree nodes (e.g. node 3), transitions less between metastable states but has increased noise when the system is <0.5.}
% \end{figure*}

% Note that for these simulations the Krackhardt kite graph was used as it shows a rich variation in the degrees of the nodes given the small network size. Crucially, the information theory approach is model free and generalizes readily to systems with other networks structures \cref{fig:other_systems}. Additionally, tests were performed replacing the kinetic Ising model with a canonical model for epidemic spreading \cref{fig:kite_res_sis}. The results show the same pattern were destabilizer nodes are essential to induce a tipping point.



% \begin{figure*}
% \centering
% \includegraphics[width=.9\linewidth]{./figures/imi_other_graphs.pdf}
% \caption{\label{fig:other_systems}Adjusted mutual information for a random tree (top), and Leder-Coxeter Fruchte graphs (middle, bottom). Each node is goverened by kinetic Ising spin dyanmics. Far away from the tipping point (fraction nodes +1 = 0.5) most information flows are concentrated on non-hub nodes. As the system approaches the tipping point (fraction = 0.5), the information flows move inwards, generating higher adjusted integrated mutual information for nodes with higher degree.}
% \end{figure*}


\section{Discussion}
\label{sec:org389dbab}
Understanding how  metastable transitions occur may  help in
understanding  how, for  example,  a pandemic  occurs, or  a
system  undergoes  critical  failure.  In  this  paper,  the
kinetic  Ising  model  was  used  to  study  how  endogenous
information  is used  to traverse  through a  tipping point.
Noise was  fixed to  maximize the statistical  complexity of
the system behavior (see \ref{sec:org8193119}). The results
show that low  degree nodes for kinetic  Ising spin dynamics
form  sources of  noise that  are propagated  as the  system
approaches the  tipping point. Crucially, the  node with the
highest  integrated mutual  information  corresponds to  the
driver-node  for   closed  systems  with   ergodic  dynamics
\cite{vanElteren2022}.   The  tipping   point  is   caused  an
\emph{information cascade}  from the  bottom-up where  low degree
nodes destabilize  higher degree nodes, slowly  allowing the
system to  climb out  the energy barrier  between metastable
states.  Closer   to  the  tipping  point,   the  asymptotic
information represents the system  ``choosing'' its new stable
state.  This leaves  an  approximated  offset, encoding  the
remaining correlation  the future system state  has with the
past  node  state  as  the   system  relaxes  into  the  new
metastable state. Together, the  information flows, lay bare
a separation of scales where  a fast-time scale dynamics are
captured  by   the  adjusted  mutual  information   and  the
approximated offset is captured by the information asymptote
(\cref{fig:kite_res,fig:kite_res_sis}).   It   is
important  to  emphasize,  that  for  the  ergodic  dynamics
considered here,  the information should decay  back to zero
due  to  the   data-processing  inequality.  The  asymptotic
information  approximates  the decay  as  an  offset as  the
slower  phase occurs  on many  order of  magnitude; that  is
after  a the  system  transitions in  to  an new  metastable
states, it remains  there for a relative  long time compared
to the  fast-time scale dynamics  (\cref{fig:introduction}
c).

\section{Conclusions}
\label{sec:org7971cd6}
The  information theoretic  approach  offers an  alternative
view   to   understand  metastable   transitions.   Adjusted
integrated  mutual   information  offers  a  novel   way  to
understand how the system  approaches, and crosses a tipping
point. The  driver node far  away from the tipping  point is
dominated by statistically more varied nodes (lower degree).
As the systems approaches the tipping point, the driver node
changes as more statistically  stable nodes are destabilized
by the lower  degree nodes. On the  tipping point, long-term
correlations stabilizes the system inside the new metastable
state. Importantly,  the information perspective  allows for
estimating integrated  mutual information  directly directly
estimated  from data  without  knowing  the mechanisms  that
drive  the  tipping  behavior.  The  results  highlight  how
short-lived  correlations  are  essential  to  initiate  the
information cascade for crossing a tipping point.

\section{Limitations}
\label{sec:org26f073f}
Adjusted integrated mutual information was computed based on
exact information flows. This  means that for binary systems
it requires  to compute  a transfer matrix  on the  order of
$2^{|S|} \times  2^{|S|}$. This  reduced the present  analysis to
smaller  graphs. It  would  be possible  to use  Monte-Carlo
methods to estimate the information flows. However, $I(s_i :
S^t)$ remains computationally expensive to compute.

In  addition, the  information approach  will only  work for
systems that lack  complete symmetry. Metastable transitions
occur  for finite-size  kinetic  Ising  models. The  current
approach will not be able  to discern node contributions due
to the internal symmetries of the system (all nodes have the
same  degree). However,  we  speculate  that the  metastable
transitions could be studied  by not controlling the tipping
point  with the  total  fraction of  nodes  in a  particular
state. In  contrast, one should  fix the system state  for a
particular region  in the  grid-graph. In this  sense, nodes
with high  variability will  destabilize more  stable nodes,
creating an  information cascade  that forces the  system to
move between metastable states.

A  general class  of  systems was  studied  governed by  the
Boltzmann-Gibbs  distribution.  For practical  purposes  the
kinetic Ising model  and SIS dynamics were  only tested, but
we speculate that the results should hold (in principle) for
other systems dictated  by the Boltzmann-Gibbs distribution.
We leave the  extension for other system  Hamiltonians up to
future work.
\section{Acknowledgments}
\label{sec:orgf30530a}
I would  like to  thank Fiona Lippert,  and Jair  Lenssen for
providing insights and feedback  in various ideas present in
this  paper. This  research is  supported by  grant Hyperion
2454972 of the Dutch National Police.

\section{References}
\label{sec:org26fe258}
\printbibliography[heading=none]

\section{Appendix}
\label{sec:org854db8e}
\subsection{Background, scope \& innovation}
\label{sec:orgd888f8c}
Noise  induced transitions  produces may  produce metastable
behavior that is fundamental  for the functioning of complex
dynamical  systems.  For  example  in  neural  systems,  the
presence   of   noise   increase   information   processing.
Similarly, the  relation between glacial ice  ages and earth
eccentricity has  been shown  to have a  strong correlation.
Metastability manifests itself by means of noise that can be
of two  kinds \cite{Forgoston2018}. External  noise originates
form   events   outside   the   internal   system   dynamics
\cite{Calim2021,Czaplicka2013a}.    Examples    include    the
influence of climate effects,  population growth or a random
noise  source  on a  transmission  line.  External noise  is
commonly modeled  by replacing an external  control or order
parameter  by  a  stochastic  process.  Internal  noise,  in
contrast, is inherent to the  system itself and is caused by
random  interactions   of  elements  of  the   system,  e.g.
individuals  in  a  population,  or  molecules  in  chemical
processes.  Both  types  of noise  can  generate  metastable
transitions between one metastable state to another. In this
paper, the metastable behavior is studied of internal noise
in complex dynamical networks  governed by the kinetic Ising
dynamics.

In this  work, a novel  approach using information  theory is
explored to  study metastable  behavior. It  offers profound
benefits  over   traditional  methods  used   in  metastable
analysis as it is model-free,  can be used for both discrete
and continuous variables, and can be estimated directly from
data  \cite{Cover2005}. Shannon  information measures  such as
mutual  information and  Fisher information  can be  used to
study how  much information  the system dynamics  share with
the  control  parameter  \cite{Nicolis2016,Lizier2010}.  These
approaches  allow  to measure  when,  for  example, a  phase
transition  occurs. However,  for  many  complex systems  an
external  control may  not be  accessible or  be absent  all
together.  In addition,  knowing about  the order  parameter
does not gain additional insight \emph{how} the system uses noise
to    transition    between     stable    points(e.g.    see
\cref{fig:introduction}).

Information  flows  may  be  used  to  study  how  a  system
transitions    between   metastable    points.   Informally,
information flow refers to the statistical coherence between
two  random processes  $X$  and $Y$  such  that the  present
information in $Y$ cannot be better explained by the past of
$X$ than  the past  of $Y$. Various  methods exist  to study
information  flow such  as transfer  entropy \cite{Schreiber},
conditional  mutual  information under  causal  intervention
\cite{Ay2008}, causation  entropy \cite{Runge2019}, time-delayed
shannon  mutual information  \cite{Li2018}  and  so on.  These
methods  are  used  to  infer the  transfer  of  information
between sets  of nodes  by possible  correcting for  a third
variable. In  a multivariate  setting most of  these methods
are prone to overestimate  or underestimate the causal flows
\cite{James2016}. In past work, the authors developed an novel
method that  reliable estimates the driver  nodes in complex
systems  using information  theory. Using  integrated mutual
information in closed ergodic  systems, the most causal node
is  exempt  from   any  spurious  statistical  correlations.
Consequently for driver nodes the information flows in these
systems   is   proportional    to   its   causal   influence
out-of-equilibrium.   Instead  of   focusing  on   a  (full)
decomposition  of statistical  variance of  source and  sinc
variables  \cite{Janzing2013,Schamberg2020,Williams2010a}, the
focus here is on understanding \emph{how} the metastable behavior
of the system occurs.

The present study innovates on prior research on information
flow and  causal node identification by  applying integrated
mutual information (IMI)  directly to metastable transitions
applies  to determine  how metastable  transitions arise  in
complex  systems  \cite{Quax2013,vanElteren2021}.  As  complex
systems are defined by a  wide variety of different types or
classes systems (e.g. open or  closed) and types of dynamics
(e.g. equilibrium  or out of equilibrium),  we restrict this
work to  systems that have probability  distributions of the
form $P(S) \propto \exp  -\beta \mathbb(S)$, where $\mathbb{H}(S)$ the
energy of  the system. In particular,  the bistable behavior
of  magnetic  spins  on  networks are  studied  dictated  by
kinetic  Ising spin  dynamics.  The kinetic  Ising model  is
considered to be one of the simplest models that shows which
shows  bistability  at  finite  size.  It  is  important  to
emphasize that  the proposed information  theoretic measures
have more implications than  merely the kinetic Ising model.
The measures can be computed  based on observations from the
systems and  can therefore  be directly estimated  from data
independent on the  underlying process or model.  The use of
kinetic Ising model  serves a convenience to  show the value
of  the proposed  method. Additionally,  it is  hypothesized
that for  metastable transitions,  short-time scales  can be
approximated using ergodic system dynamics.

\subsection{Methods \& definitions}
\label{sec:org8193119}
\subsubsection{Model}
\label{sec:org5382bb5}
To  study metastable  behavior, we  consider a  system as  a
collection of random variables $S = \{s_1, \dots, s_n\}$ governed
by the Boltzmann-Gibbs distribution

\begin{equation}
P(S)    =     \frac{1}{Z}    \exp(- \beta \mathcal{H}(S) ),
\end{equation}

where  is the  inverse temperature  $\beta =  \frac{1}{T}$ which
control  the  noise in  the  system,  $\mathcal{H}(S)$ is  the  system
Hamiltonian which encodes the node-node dynamics. The choice
of the energy function dictates what kind of system behavior
we observe. Here, we focus on arguable the simplest models that
shows metastable behavior: the  kinetic Ising model, and the
Susceptible-Infected-Susceptible model.

Temporal  dynamics  are simulated  using Glauber dynamics
sampling.  In each  discrete time  step a  spin is  randomly
chosen and a new state $X'\in S$ is accepted with probability

\begin{equation}
\label{eq:glauber}
p( \text{accept} X' ) = \frac{1}{1 + \exp(-\beta \Delta E)},
\end{equation}
where $\Delta E = \mathcal{H}(X') -  \mathcal{H}(X)$ is the energy difference between
the current state $X$ and the proposed state $X'$.


\subsubsection{Kinetic Ising model}
\label{sec:orgb324012}
The  traditional Ising  model  was  originally developed  to
study ferromagnetism, and is  considered one of the simplest
models that generate complex behavior.  It consists of a set
of binary distributed  spins $S = \{s_1,  \dots s_n\}$. Each
spin contains energy given by the Hamiltonian

\begin{equation}
\label{eq:energy}
\begin{split}
    \mathcal{H}(S) = -\sum_{i,j} J_{ij} s_{i} s_{j} - h_{i} s_{i}.
\end{split}
\end{equation}
where $J_{ij}$ is the interaction  energy of the spins $s_i,
s_j$.  The   interaction  energy  effectively   encodes  the
underlying  network  structure   of  the  system.  Different
network  structures are  used  in this  study  to provide  a
comprehensive  numerical overview  of  the relation  between
network  structure and  information flows  (see \ref{sec:org8193119}). The interaction energy  $J_{ij}$ is set to 1
if a connection exists in the network.


For sufficiently  low noise  (temperature), the  Ising model
shows metastable behavior (\cref{fig:introduction} c). Here,
we  aim to  study \emph{how}  the system  goes through  a tipping
point by  tracking the  information flow  per node  with the
entire system state.

\subsubsection{SIS model}
\label{sec:orgddc3122}
The  SIS  model is  arguable  the  simplest model  to  study
epidemic spreading. Each agent can either be susceptible (0)
or infected (1). The  agents can transition from susceptible
to infected proportional to the number of infected people it
is in contact with. In addition,  each agent has a base rate
of becoming  infectious. One  can describe the  SIS dynamics
using the Hamiltonian as:

\begin{equation}
\label{eq:sis}
\mathcal{H}(S)_{SIS} = \sum_i (2s_i - 1) (1 - \eta)^{\sum_{j} A_{ij} s_j} - \mu s_i,
\end{equation}
where $\eta$ is  the infection rate, $\mu$ is  the recovery rate,
and $A_{ij}$ is 1 if $s_i$ and $s_j$ have an interaction, 0
otherwise.

\subsection{Information flow on complex networks}
\label{sec:org3d3e541}
Informally, the  information flows measures  the statistical
coherence between two random variables $X$ and $Y$ over time
such that the present information in $Y$ cannot be explained
by the past of $Y$ but rather by the past of $X$. Estimating
information flow is inherently difficult due to the presence
of confounding  which potential traps the  interpretation in
the  ``correlation  does  not equal  causation''.  Under  some
context,  however, information  flow can  be interpreted  as
causal \cite{vanElteren2021}. Let $S=\{s_1, \dots, s_n\}$ be a
random process, and $S^t$ represent  the state of the random
process at some time $t$.  The information present in $S$ is
given as the Shannon entropy

\begin{equation}
H(S) = \sum_{x \in S} p(x) \log p(x)
\end{equation}

where $\log$ is  base 2 unless otherwise  stated, and $p(x)$
is  used as  a short-hand  for $p(S  = x)$.  Shannon entropy
captures the  uncertainty of  a random  variable; it  can be
understood  as  the number  of  yes/no  questions needed  to
determine  the state  of  $S$. This  measure of  uncertainty
naturally  extends  to  two variables  with  Shannon  mutual
information. Let  $s_i$ be an  element of the state  of $S$,
then the Shannon mutual information $I(S; s_i)$ is given as

\begin{equation}
\begin{aligned}
I(X; Y) &= \sum_{x\in S, y \in s_i} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
        &= H(S) - H(S | s_i)
\end{aligned}
\end{equation}

Shannon  mutual   information  can  be  interpreted   as  the
uncertainty reduction of $S$ after knowing the state of $s_i$.
Consequently, it encodes how  much statistical coherence $s_i$
and $S$ share.  Shannon mutual  information can be
measured over time to  encode how  much \emph{information} (in  bits) flows
from state $s_i$ to $S^{t}$

\begin{equation}
\label{eq:flow}
\begin{aligned}
I(S^t; s_i) = H(S^t) - H(S^t | s_i).
\end{aligned}
\end{equation}


Prior results showed that the  nodes with the highest causal
importance are those nodes that have the highest information
flow   (i.e.   maximize  \ref{eq:flow})   \cite{vanElteren2022}.
Intuitively,  the   nodes  for   which  the   future  system
``remembers'' information from a node  in the past, is the one
that ``drives''  the system  dynamics. Formally,  these driver
nodes can  be identified by computing  the total information
flow  between  $S^t$ and  $s_i$  can  be captured  with  the
integrated mutual information \cite{vanElteren2021}

\begin{equation}
\label{eq:imi}
\mu(s_i) = \sum_{\tau = 0}^{\infty} I(s_{i}^{t-\tau} ; S^t).
\end{equation}

The  driver nodes  are the  nodes that  maximize \ref{eq:imi}.
Note  that  in  \cite{vanElteren2022}  $I(S  :  s_i^{t})$  was
considered.   Here,    information   flows    are   computed
out-of-equilibrium  with  symmetry  breaking. That  is,  the
system  dynamics are  evolved by  starting the  system at  a
distance   from   the   tipping  point   and   evolving   it
out-of-equilibrium. This causes $I(s_i^t : S)$ to not follow
the data processing inequality  as information may flow back
into a  node. The choice  for computing $I(s_i^t :  S)$ over
$I(s_i  : S^t)$  was done  for computational  feasibility in
\cite{vanElteren2022}.   Furthermore,   the  data   processing
inequality  was  not  violated when  considered  the  system
without   symmetry  breaking.   For  \ref{eq:flow}   the  data
processing   inequality  is   guaranteed,   however  it   is
computationally more challenging to compute (see \ref{sec:org26f073f}).
\subsection{Noise matching procedure}
\label{sec:org11ee4e3}
The Boltzmann-Gibbs  distribution is parameterized  by noise
factor $\beta =  \frac{1}{kT}$ where $T$ is  the temperature and
$k$  is   the  Boltzmann  constant.  For   high  $\beta$  values
metastable behavior  occurs in the kinetic  Ising model. The
temperature was chosen such  that the statistical complexity
\cite{Lopez-Ruiz1995a}   was    maximized.   The   statistical
complexity $C$ is computed as

\begin{equation}
C = \bar H(S) D(S),
\end{equation}

where $\bar H(S) = \frac{H(s)}{-\log_2(|S|)}$ is the system  entropy, and $D(S)$ measures the
distance to disequilibrium

\begin{equation}
D(S) = \sum_i (p(S_i) - \frac{1}{|S|})^2.
\end{equation}

A   typical  statistical   complexity  curve   is  seen   in
\cref{fig:stat_compl}. The noise parameter $\beta$ is set such that
it  maximizes  the  statistical complexity  using  numerical
optimization (COBYLA method in scipy's \texttt{optimize.minimize}
module) \cite{Virtanen2020}.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/exact_kite_dyn=ising_beta=0.5732374683235916_T=200_statistical_complexity.png}
\caption{\label{fig:stat_compl}(a) Statistical complexity ($C$), normalized system entropy ($H(S)$) and disequilibrium ($D(S)$) as a function of the temperature ($T = \frac{1}{\beta}$) for Krackhardt kite graph. The noise parameter was set such that it maximizes the statistical complexity (vertical black line). The values are normalized between [0,1] for aesthetic purposes. (b) State distribution $P(S)$ for temperature that maximizes the statistical complexity in (a) as a function of nodes in state +1.}
\end{figure}

\subsection{Exact information flows $I(s_i ; S^t)$}
\label{sec:org59af222}
In   order   to   compute   $\infdecay$,   the   conditional
distribution  $p(S^t  |  s_i)$  and  $p(S^t)$  needs  to  be
computed. For  Glauber dynamics, the system  $S$ transitions
into $S'$ by considering to  flips by randomly choosing node $s_i$. The  transition matrix $P(S^t |  s_i) = \m P$  can be
constructed by computing each entry $p_{ij}$ as

\begin{equation}
\label{eq:glauber}
\begin{split}
p_{ij, i \neq j} &= \frac{1}{|S|} \frac{1}{ 1 + \exp (-\Delta E) }\\
p_{ii} &= 1 - \sum_{j, j \neq i} P_{ij},
\end{split}
\end{equation}

where $\Delta E =  \mathcal{H}(S_j) - \mathcal{H}(S_j)$ encodes the
energy difference of  moving from $S_i$ to  $S_j$. The state
to state transition $\m P$ matrix will be of size $2^{|S|} \times
2^{|S|} \times |\mathcal{A}_{s_i}|$, where $|\mathcal{A}_{s_i}|$ is the size of
the alphabet of $s_i$, which becomes computationally intractable due
to its  exponential growth with  the system size  $|S|$. The
exact information  flows can then be  computed by evaluating
$p(S^t |  s_i)$ out of  equilibrium by evaluating  all $S^t$
for  all  possible  node  states  $s_i$  where  $p(S^t)$  is
computed as

\begin{equation}
p(S^t) = \sum_{s_i} p(S^t | s_i ) p(s_i).
\end{equation}

\subsection{White noise estimation procedure}
\label{sec:orgc093508}
Let $M(S^t)$  represent the instantaneous  system macrostate
compute as the system average

\begin{equation}
\label{}
M(S^t) = \frac{1}{|S|} \sum_i s_i.
\end{equation}

The metastable behavior is characterized for the Ising model
by the system fluctuating around two stable points $T1$ with
$M(S) \approx 0$ and $T2$ with $M(S) \approx 1$ for most of the time. By
pinning intervention  the node  is pinned  to the  +0 state,
effectively  biasing the  macrostate $M(S^t)$  towards $T1$.
For any  particular trajectory  the fluctuations  around the
stable points contributed differently for nodes depending on
the nodes embedness  in the system; lower  degree nodes tend
to produce higher fluctuations than higher degree nodes (see
main text). We define the  fluctuations as ``white noise'' and
characterize  the white  noise  as function  of the  pinning
intervention  on   different  nodes.  The  white   noise  is
characterized by  first computing  the set  of windows  $W =
\{w_i  | i\in  1,  \dots  n\}$ where  each  window  $w_i \in  W$
represents  the duration  in  a trajectory  that the  system
state stayed  in either $T1$  or $T2$. Next, the  white noise
was characterized as

\begin{equation}
\label{}
\eta = \frac{1}{|W|} \sqrt{ \sum_j^{|w_i|} (w_{ij} - \overline{w_i})^2},
\end{equation}

where $\overline{w_i}$  is the  average of window  $w_i$ of
the instantaneous  macrostate and  $w_{ij}$ is  a particular
instantaneous state. The white  noise was estimated for $T1$
and $T2$ separately and displayed in \cref{fig:kite_noise}.

\subsection{Switch susceptibility as a function of degree}
\label{sec:org009e10c}
First,  we investigate  the susceptibility  of a  spin as  a
function  of  its  degree.  The  susceptibility  of  a  spin
switching  its  state  is  a function  both  of  the  system
temperature $T$ and the system dynamics. The system dynamics
would   contribute  to   the   susceptibility  through   the
underlying network structure  either directly or indirectly.
The  network  structure  produces local  correlations  which
affects the switch probability for a given spin.

As an initial approximation,  we consider the susceptibility
of a  target spin $s_i$ to  flip from a majority  state to a
minority state  given the state  of its neighbors  where the
neighbors are  not connected among themselves.  Further, the
assumption is that for the instantaneous update of $s_i$ the
configuration of the neighborhood of $s_i$ can be considered
as the  outcome of a  binomial trial.  Let, $N$ be  a random
variable with state space $\{0,  1\}^{|N|}$, and let $n_j \in N$
represent a neighbor of $s_i$.  We assume that all neighbors
of  $s_i$ are  i.i.d.  distributed  given the  instantaneous
system magnetization

\begin{equation}
M(S^t) = \frac{1}{|S^t|} \sum_i s_i^t.
\end{equation}

Let the minority  state be 1 and the majority  state be 0,
the expectation of $s_i$ flipping from the majority state to
the minority state is given as:

\begin{align}
\label{majority_flip}
    \begin{aligned}
        E[ p(s_i = 1 | N ) ]_{p(N)} &= \sum_{N_i \in N} p(N_i) p(s_i = 1 | N_i)\\
            &= \sum_{N_i \in  N} \prod_j^{|N_i|} p(n_j) p(s_i  = 1 |N_i)\\
            &=  \sum_{N_i \in N}  {n\choose k} f^k  (1  -
            f)^{n-k}  p(s_i  = 1 | f), \\
    \end{aligned}
\end{align}

where $f$ is  the fraction of nodes in  the majority states,
$n$ is the  number of neighbors, $k$ is the  number of nodes
in state 0. In figure \cref{fig:maj_flip}. this is computed
as a  function of the  degree of  spin $s_i$. As  the degree
increases,   the  susceptibility   for   a  spin   decreases
relatively  to  the same  spin  with  a lower  degree.  This
implies  that   the  susceptibility  of  change   to  random
fluctuations are  more likely  to occur  in nodes  with less
external constraints as measured by degree.

%CvE Figuur hier geplaatst om de hoofdtekst wat helderder te houden
\section{Information flows with SIS  dynamics}

\begin{figure*}
\centering
\includegraphics[width=.9\linewidth]{./figures/sis_kite_graph.pdf}
\caption{\label{fig:kite_res_sis}(a) As the system approaches the tipping point the information processing moves from lower degree  nodes to higher degree nodes. Each node is governed by Suseptible-Infective-Susceptible dynamics with infection rate = 0.1, and recovery rate = 0.1. The node size is proportional to the adjusted integrated mutual information. (b) Information flows as a function of system stability. Far from the tipping point the information processing is mainly in lower degree nodes. As the system approaches the tipping point, the information flows increases for all nodes. Higher degree nodes tend to have higher adjusted integrated mutual information and higher information offset. The information offset encodes the long-time scale correlation of the node with the system state. A higher asymptotic information implies that the system remembers the node state for longer than other nodes.}
\end{figure*}



\section{Flip probability per degree}
\label{sec:deg_flip}
%CvE: "semi-meanfield" is nu niet meer deel van de main text
In \cref{fig:maj_flip}  the tendency for a  node to flip
from  the majority  to  the minority  state  is computed  as
function of fraction of nodes possessing the majority states
+1 in the  system, denoted as $N$. Two  things are observed.
First, nodes with lower degree are more susceptible to noise
than nodes with higher degree.  For a given system stability,
nodes with lower degree tend  to have a higher tendency
to flip. This is true for all distances of the system to the
tipping point.  In contrast,  the higher  the degree  of the
node, the closer the system has to be to a tipping point for
the node to  change its state. This can be  explained by the
fact that lower degree nodes, have fewer constraints compared
to nodes with higher degree  nodes. For Ising spin kinetics,
the nodes  with higher  degree tend to  be more  ``frozen'' in
their node dynamics than nodes with lower degree. Second, in
order for a node to flip with probability with similar mass,
i.e. ($E[p(s_i) |  N] = 0.2$) a node with  higher degree needs
to  be closer  to the  tipping point  than nodes  with lower
degree. In  fact, the order of  susceptibility is correlated
with   the  degree;   the   susceptibility  decreases   with
increasing degree and fixed fraction of nodes in state 1.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{./figures/fig_majority_flip.pdf}
\caption{\label{fig:maj_flip}Susceptibility of a node with degree $k$ switching from the minority state 0 to the majority state 1 as a function of the neighborhood entropy for $\beta = 0.5$. The neighborhood entropy encodes how stable the environment of a spin is. As the system approaches the tipping point, the propensity of a node to flip from to the minority state increases faster for low degree nodes than for high degree nodes. Higher degree nodes require more change in their local environment to flip to the majority state. See for details \ref{sec:org009e10c}.}
\end{figure}
\end{document}
