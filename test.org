* Introduction
:PROPERTIES:
:CUSTOM_ID: sec:orgd6a1d62
:END:
Complex dynamical systems such as cell-regulatory processes,
epidemic  spreading   dynamics,  opinion   formation  neural
behavior and so on, often exhibit extended periods of robust
systemic behavior. By correlating fluctuations, networks can
endogenously  induce  a  rapid transition  from  one  stable
systemic behavior to  another. These so-called noise-induced
events  are often  rare events  and may  be associated  with
either desirable or undesirable outcome such as the onset or
extinction  of  an  infectious   disease  or  the  onset  of
epileptic   seizure.   The   mechanisms   underlying   these
transitions are  often not well  understood. It is  of vital
importance to understand the underlying processes that cause
metastable  behavior  to quantify  the  impact  of noise  on
complex systems.

Here, we  consider dynamical systems consisting  of a static
network  where the  states of  the nodes  are governed  by a
Boltzmann-Gibbs distribution.  This type  of model  has been
used to  describe a wide  range of behaviors such  as neural
dynamics     [cite:@cite_needed],      opinion     dynamics,
ferromagnetic   spins  [cite:@Glauber1963],   and  organized
criminal  gang interactions  [cite:@DOrsogna2015a]. In  this
class of models, each node  chooses its state in equilibrium
with  the potential  induced by  its neighboring  states. In
physical applications  this potential is the  classic energy
potential, but in other  applications it can be interpreted,
for  instance  as  frustration  level,  homophily,  or  more
broadly speaking,  a fitness score  of the node  state given
its  neighbors. The  second ingredient  in this  model is  a
global 'temperature' which is  essentially a noise level: at
zero noise a  node always picks the  absolute minimum energy
state, whereas the higher this  noise level, the more likely
it is that high energy states are chosen.

These models  lead generally to metastable  behavior because
there are  usually multiple  (local) minima in  the system's
potential. In finite systems and non-zero temperature, there
is a  finite probability that the  system moves (eventually)
from  one   local  minimum  to  another.   Without  loss  of
generality, in this paper we illustrate our method using the
well-known kinetic Ising spin model without external forces.
Here, nodes  have only  two possible states:  +0 and  +1. At
system level there are two global minima: all nodes in state
+0 or all nodes in state +1. Between these two system states
lies a  'potential barrier':  many possible paths  of system
states which  connect the  two systemic  minima, but  all of
which  have a  growing  potential, making  these paths  less
likely than paths of similar length that remain close to one
of  the  minima.  The  peak of  this  potential  along  each
crossing path  lies informally  speaking at  a 'checkerboard
pattern':  each node  being  maximally  different from  (the
majority of)  its neighbors.  We refer to  this peak  as the
'tipping point'. See [[#fig:introduction][[fig:introduction]]].

The crucial  point here  is that  the network  structure can
make systemic transitions much  more likely than without it.
Without  network  effects,  each  node  has  an  independent
probability of choosing the (unlikely) high potential state.
The probability  that all nodes  in the system happen  to do
this simultaneously (thereby  transitioning the system state
to another potential minimum)  decreases to zero rapidly (as
\(\mathcal{O}(e^{-N^2})\)  for   dense  networks).  This   means  that
transitions  become  unlikely  for   all  but  the  smallest
systems.  With  network  effects, however,  transitions  can
potentially occur along  a path of nodes that  form a domino
effect. That  is, the  first node  choosing randomly  a high
potential state makes the  same state transition more likely
for all of  its neighbors. For some of  these neighbors this
new situation may  suffice to make the  same transition with
(almost)  equal likelihood  as the  first node,  and so  on,
until the tipping point has  been reached. The likelihood of
such  a  transition  is  much higher  than  without  network
effects   (up   to   \(\mathcal{O}(e^{-N})\)).  This   is   still   an
exponentially decaying function of system size, highlighting
the  fact that  such  noise-induced  transitions still  only
occur in finite-size systems, but exceedingly more likely.

Here we present a method  to uncover the network percolation
process    that   facilitates    endogenous,   noise-induced
transitions. The  computational method only  requires access
to  cross-sections of  time-series  of  observations of  the
system, meaning that it is broadly applicable.

The  method consists  of  analyzing two  key features  using
information flows  of a system:  the time of  the short-term
information  decay,  and  the long-term  information  level.
Here, the contribution  of each node to  the system dynamics
over  time  are  considered.   The  results  highlight  that
short-lived   correlations   measured  by   Shannon   mutual
information shared  between and  node and the  entire system
(\(I(s_i : S^t\)) are essential to absorb and transfer noise
through the system. After the majority of the system crosses
the tipping  point, a new local  equilibrium is established.
These long-term correlations are essential for the system to
maintain  its metastable  state. The  approach differs  from
traditional approaches  that focus  on how  the system  as a
whole  approaches  a  tipping  point.  Here,  the  mechanism
underlying /how/  local connectivity of nodes  contribute to
the system dynamics can be understood and analyzed.

[[./figures/figure1.png]]

* Results
:PROPERTIES:
:CUSTOM_ID: sec:org6f0b033
:END:
a-c shows a graphical representation of how a network of binary
distribution governed by
[[#eq:glauber,eq:energy][[eq:glauber,eq:energy]]] can lead to
bistability of the system macrostate. Some configurations of the system
are more stable than other configurations
([[#fig:introduction][[fig:introduction]]]b). As the system evolves over
time, the system favors macrostates with higher stability.

In [[#fig:introduction][[fig:introduction]]]c an example system
trajectory is shown. Three markers indicate distinct states the system
may be in. The green markers indicates and example where the system far
away from the tipping point. The macrostate of the system is close to
the stable state with all the nodes are in the +0 state. As the system
evolves, local transient perturbations will destabilize the system
macrostate. Most destabilization relax back to the meteastable point.
For some noise-induced perturbations, however, the system macrostate
rapidly transitions between metastable points (for example the green to
blue to red circle transition).

The contribution of each node to the macrostate is equal, however the
node contribution is not equal in terms for the macrostate reaching the
tipping point. Consider for example a node with low degree, e.g. degree
1, and a node with high degree, e.g. degree 10 ([[#fig:maj_flip][2]]). A
node with lower degree is more likely to flip given the state of its
neighbors than a node with higher degree. Consequently, it is more
likely that nodes with lower degree destabilize the system, pushing the
system closer to the tipping point.

From an information perspective, the contribution of the dynamics of a
node can be quantified using time-delayed Shannon information. Depending
on the connectivity of a node in the system, the contribution to the
system macrostate will differ [cite:@vanElteren2022; @Quax2013]. How
much the future system state is affected by the node's current state is
computed by shared information with the node's current state \(s_i\) and
the future system state \(S^t\) as the adjusted mutual integrated
information

\[\label{eq:adj_imi}
\bar \mu_(s_i) = \sum_{t = 0}^\infty (I(s_i : S^t) - \omega_{s_i}) \Delta t.\]

Intuitively \(\mu(s_i)\) represents the transient dynamics of how much
the influence of a node is "remembered" by the system over time. It
reflects how the effects of local dynamics between nodes percolates
through the system over time. As the system chooses it next metastable
state, the system macrostate is dominated by transient dynamics. The
next tipping point will be reached on a much longer time-scale.
Consequently, \(\omega\) quantifies the system returning to a stable
system regime. For nodes with fast dynamics, \(\mu(s_i)\) is generally
high and \(\omega_{s_i}\) would be generally low

In [[#fig:introduction][[fig:introduction]]]d-f the information flows
are computed that quantifies the contribution of a node on the future
system state. The information flows are computed by considering the
system in different states of stability (see
[[#appendix ref][[appendix ref]]]). That is, the subset of states are
selected such that states
\(S_{\gamma} = \{S' \subseteq S | M(S) = \gamma\}\) where \(\gamma\) is
the fraction of nodes having state +1. By evolving all possible
trajectories, the exact information flows are computed for \(t=500\)
steps. Asymptotic and integrated mutual information are estimated using
regression ([[#appendix ref][[appendix ref]]]).

Three things are observed. First, far away from the tipping point
([[#fig:kite_res][[fig:kite_res]]]a, nodes with lower degree are more
dynamics yielding higher \(\mu(s_i)\). In the stable regime, the nodes
with lower degree that tend to destabilize the system flipping from
majority state to the minority state. As the system approaches the
tipping point, the transient dynamics of higher degree nodes increases.
Second, the asymptotic information remains low far away from the tipping
point, and becomes increasingly high as the system approaches the
tipping point ([[#fig:introduction][[fig:introduction]]]e,f and
[[#fig:kite_res][[fig:kite_res]]]b). The increase of the asymptotic
information reflects the likelihood of the system escaping the local
metastable state and transitioning to another metastable state. For
example, the green hub node in [[#fig:kite_res][[fig:kite_res]]] has
maximum asymptotic information at the tipping point. After choosing the
next state, the hub node can be used as a proxy to determine on which
side of the tipping point remains.

The tipping point is reached by a sequence of flips starting from the
nodes with the highest potential to flip. In
[[#fig:max_trajectory][[fig:max_trajectory]]] a trajectory is shown that
maximizes \(\log P(S^t|S^{t-1}, M(S^5) = 0.5)\). The system starts in
the stable regime with all nodes having state +0. The tipping point is
reached by the lowest degree node flipping first, which promotes the
probability of its neighbor flipping.

* Discussion
:PROPERTIES:
:CUSTOM_ID: sec:org389dbab
:END:
Understanding how metastable transitions occur may help in understanding
how, for example, a pandemic occurs, or a system undergoes critical
failure. In this paper, the kinetic Ising model was used to study how
endogenous information is used to traverse through a tipping point.
Noise was fixed to maximize the statistical complexity of the system
behavior (see [[#sec:org8193119][8.2]]). The results show that low
degree nodes for kinetic Ising spin dynamics form sources of noise that
are propagated as the system approaches the tipping point. Crucially,
the node with the highest integrated mutual information corresponds to
the driver-node for closed systems with ergodic dynamics
[cite:@vanElteren2022]. The tipping point is caused an /information
cascade/ from the bottom-up where low degree nodes destabilize higher
degree nodes, slowly allowing the system to climb out the energy barrier
between metastable states. Closer to the tipping point, the asymptotic
information represents the system "choosing" its new stable state. This
leaves an approximated offset, encoding the remaining correlation the
future system state has with the past node state as the system relaxes
into the new metastable state. Together, the information flows, lay bare
a separation of scales where a fast-time scale dynamics are captured by
the adjusted mutual information and the approximated offset is captured
by the information asymptote
([[#fig:kite_res,fig:kite_res_sis][[fig:kite_res,fig:kite_res_sis]]]).
It is important to emphasize, that for the ergodic dynamics considered
here, the information should decay back to zero due to the
data-processing inequality. The asymptotic information approximates the
decay as an offset as the slower phase occurs on many order of
magnitude; that is after a the system transitions in to an new
metastable states, it remains there for a relative long time compared to
the fast-time scale dynamics ([[#fig:introduction][[fig:introduction]]]
c).

* Conclusions
:PROPERTIES:
:CUSTOM_ID: sec:org7971cd6
:END:
The information theoretic approach offers an alternative view to
understand metastable transitions. Adjusted integrated mutual
information offers a novel way to understand how the system approaches,
and crosses a tipping point. The driver node far away from the tipping
point is dominated by statistically more varied nodes (lower degree). As
the systems approaches the tipping point, the driver node changes as
more statistically stable nodes are destabilized by the lower degree
nodes. On the tipping point, long-term correlations stabilizes the
system inside the new metastable state. Importantly, the information
perspective allows for estimating integrated mutual information directly
directly estimated from data without knowing the mechanisms that drive
the tipping behavior. The results highlight how short-lived correlations
are essential to initiate the information cascade for crossing a tipping
point.

* Limitations
:PROPERTIES:
:CUSTOM_ID: sec:org26f073f
:END:
Adjusted integrated mutual information was computed based on exact
information flows. This means that for binary systems it requires to
compute a transfer matrix on the order of \(2^{|S|} \times  2^{|S|}\).
This reduced the present analysis to smaller graphs. It would be
possible to use Monte-Carlo methods to estimate the information flows.
However, \(I(s_i :
S^t)\) remains computationally expensive to compute.

In addition, the information approach will only work for systems that
lack complete symmetry. Metastable transitions occur for finite-size
kinetic Ising models. The current approach will not be able to discern
node contributions due to the internal symmetries of the system (all
nodes have the same degree). However, we speculate that the metastable
transitions could be studied by not controlling the tipping point with
the total fraction of nodes in a particular state. In contrast, one
should fix the system state for a particular region in the grid-graph.
In this sense, nodes with high variability will destabilize more stable
nodes, creating an information cascade that forces the system to move
between metastable states.

A general class of systems was studied governed by the Boltzmann-Gibbs
distribution. For practical purposes the kinetic Ising model and SIS
dynamics were only tested, but we speculate that the results should hold
(in principle) for other systems dictated by the Boltzmann-Gibbs
distribution. We leave the extension for other system Hamiltonians up to
future work.

* Acknowledgments
:PROPERTIES:
:CUSTOM_ID: sec:orgf30530a
:END:
I would like to thank Fiona Lippert, and Jair Lenssen for providing
insights and feedback in various ideas present in this paper. This
research is supported by grant Hyperion 2454972 of the Dutch National
Police.

* References
:PROPERTIES:
:CUSTOM_ID: sec:org26fe258
:END:
* Appendix
:PROPERTIES:
:CUSTOM_ID: sec:org854db8e
:END:
** Background, scope & innovation
:PROPERTIES:
:CUSTOM_ID: sec:orgd888f8c
:END:
Noise induced transitions produces may produce metastable behavior that
is fundamental for the functioning of complex dynamical systems. For
example in neural systems, the presence of noise increase information
processing. Similarly, the relation between glacial ice ages and earth
eccentricity has been shown to have a strong correlation. Metastability
manifests itself by means of noise that can be of two kinds
[cite:@Forgoston2018]. External noise originates form events outside the
internal system dynamics [cite:@Calim2021; @Czaplicka2013a]. Examples
include the influence of climate effects, population growth or a random
noise source on a transmission line. External noise is commonly modeled
by replacing an external control or order parameter by a stochastic
process. Internal noise, in contrast, is inherent to the system itself
and is caused by random interactions of elements of the system, e.g.
individuals in a population, or molecules in chemical processes. Both
types of noise can generate metastable transitions between one
metastable state to another. In this paper, the metastable behavior is
studied of internal noise in complex dynamical networks governed by the
kinetic Ising dynamics.

In this work, a novel approach using information theory is explored to
study metastable behavior. It offers profound benefits over traditional
methods used in metastable analysis as it is model-free, can be used for
both discrete and continuous variables, and can be estimated directly
from data [cite:@Cover2005]. Shannon information measures such as mutual
information and Fisher information can be used to study how much
information the system dynamics share with the control parameter
[cite:@Nicolis2016; @Lizier2010]. These approaches allow to measure
when, for example, a phase transition occurs. However, for many complex
systems an external control may not be accessible or be absent all
together. In addition, knowing about the order parameter does not gain
additional insight /how/ the system uses noise to transition between
stable points(e.g. see [[#fig:introduction][[fig:introduction]]]).

Information flows may be used to study how a system transitions between
metastable points. Informally, information flow refers to the
statistical coherence between two random processes \(X\) and \(Y\) such
that the present information in \(Y\) cannot be better explained by the
past of \(X\) than the past of \(Y\). Various methods exist to study
information flow such as transfer entropy [cite:@Schreiber], conditional
mutual information under causal intervention [cite:@Ay2008], causation
entropy [cite:@Runge2019], time-delayed shannon mutual information
[cite:@Li2018] and so on. These methods are used to infer the transfer
of information between sets of nodes by possible correcting for a third
variable. In a multivariate setting most of these methods are prone to
overestimate or underestimate the causal flows [cite:@James2016]. In
past work, the authors developed an novel method that reliable estimates
the driver nodes in complex systems using information theory. Using
integrated mutual information in closed ergodic systems, the most causal
node is exempt from any spurious statistical correlations. Consequently
for driver nodes the information flows in these systems is proportional
to its causal influence out-of-equilibrium. Instead of focusing on a
(full) decomposition of statistical variance of source and sinc
variables [cite:@Janzing2013; @Schamberg2020; @Williams2010a], the focus
here is on understanding /how/ the metastable behavior of the system
occurs.

The present study innovates on prior research on information flow and
causal node identification by applying integrated mutual information
(IMI) directly to metastable transitions applies to determine how
metastable transitions arise in complex systems
[cite:@Quax2013; @vanElteren2021]. As complex systems are defined by a
wide variety of different types or classes systems (e.g. open or closed)
and types of dynamics (e.g. equilibrium or out of equilibrium), we
restrict this work to systems that have probability distributions of the
form \(P(S) \propto \exp  -\beta \mathbb(S)\), where \(\mathbb{H}(S)\)
the energy of the system. In particular, the bistable behavior of
magnetic spins on networks are studied dictated by kinetic Ising spin
dynamics. The kinetic Ising model is considered to be one of the
simplest models that shows which shows bistability at finite size. It is
important to emphasize that the proposed information theoretic measures
have more implications than merely the kinetic Ising model. The measures
can be computed based on observations from the systems and can therefore
be directly estimated from data independent on the underlying process or
model. The use of kinetic Ising model serves a convenience to show the
value of the proposed method. Additionally, it is hypothesized that for
metastable transitions, short-time scales can be approximated using
ergodic system dynamics.

** Methods & definitions
:PROPERTIES:
:CUSTOM_ID: sec:org8193119
:END:
*** Model
:PROPERTIES:
:CUSTOM_ID: sec:org5382bb5
:END:
To study metastable behavior, we consider a system as a collection of
random variables \(S = \{s_1, \dots, s_n\}\) governed by the
Boltzmann-Gibbs distribution

\[P(S)    =     \frac{1}{Z}    \exp(- \beta \mathcal{H}(S) ),\]

where is the inverse temperature \(\beta =  \frac{1}{T}\) which control
the noise in the system, \(\mathcal{H}(S)\) is the system Hamiltonian
which encodes the node-node dynamics. The choice of the energy function
dictates what kind of system behavior we observe. Here, we focus on
arguable the simplest models that shows metastable behavior: the kinetic
Ising model, and the Susceptible-Infected-Susceptible model.

Temporal dynamics are simulated using Glauber dynamics sampling. In each
discrete time step a spin is randomly chosen and a new state \(X'\in S\)
is accepted with probability

\[\label{eq:glauber}
p( \text{accept} X' ) = \frac{1}{1 + \exp(-\beta \Delta E)},\] where
\(\Delta E = \mathcal{H}(X') -  \mathcal{H}(X)\) is the energy
difference between the current state \(X\) and the proposed state
\(X'\).

*** Kinetic Ising model
:PROPERTIES:
:CUSTOM_ID: sec:orgb324012
:END:
The traditional Ising model was originally developed to study
ferromagnetism, and is considered one of the simplest models that
generate complex behavior. It consists of a set of binary distributed
spins \(S = \{s_1,  \dots s_n\}\). Each spin contains energy given by
the Hamiltonian

\[\label{eq:energy}
\begin{split}
    \mathcal{H}(S) = -\sum_{i,j} J_{ij} s_{i} s_{j} - h_{i} s_{i}.
\end{split}\] where \(J_{ij}\) is the interaction energy of the spins
\(s_i,
s_j\). The interaction energy effectively encodes the underlying network
structure of the system. Different network structures are used in this
study to provide a comprehensive numerical overview of the relation
between network structure and information flows (see
[[#sec:org8193119][8.2]]). The interaction energy \(J_{ij}\) is set to 1
if a connection exists in the network.

For sufficiently low noise (temperature), the Ising model shows
metastable behavior ([[#fig:introduction][[fig:introduction]]] c). Here,
we aim to study /how/ the system goes through a tipping point by
tracking the information flow per node with the entire system state.

*** SIS model
:PROPERTIES:
:CUSTOM_ID: sec:orgddc3122
:END:
The SIS model is arguable the simplest model to study epidemic
spreading. Each agent can either be susceptible (0) or infected (1). The
agents can transition from susceptible to infected proportional to the
number of infected people it is in contact with. In addition, each agent
has a base rate of becoming infectious. One can describe the SIS
dynamics using the Hamiltonian as:

\[\label{eq:sis}
\mathcal{H}(S)_{SIS} = \sum_i (2s_i - 1) (1 - \eta)^{\sum_{j} A_{ij} s_j} - \mu s_i,\]
where \(\eta\) is the infection rate, \(\mu\) is the recovery rate, and
\(A_{ij}\) is 1 if \(s_i\) and \(s_j\) have an interaction, 0 otherwise.

** Information flow on complex networks
:PROPERTIES:
:CUSTOM_ID: sec:org3d3e541
:END:
Informally, the information flows measures the statistical coherence
between two random variables \(X\) and \(Y\) over time such that the
present information in \(Y\) cannot be explained by the past of \(Y\)
but rather by the past of \(X\). Estimating information flow is
inherently difficult due to the presence of confounding which potential
traps the interpretation in the "correlation does not equal causation".
Under some context, however, information flow can be interpreted as
causal [cite:@vanElteren2021]. Let \(S=\{s_1, \dots, s_n\}\) be a random
process, and \(S^t\) represent the state of the random process at some
time \(t\). The information present in \(S\) is given as the Shannon
entropy

\[H(S) = \sum_{x \in S} p(x) \log p(x)\]

where \(\log\) is base 2 unless otherwise stated, and \(p(x)\) is used
as a short-hand for \(p(S  = x)\). Shannon entropy captures the
uncertainty of a random variable; it can be understood as the number of
yes/no questions needed to determine the state of \(S\). This measure of
uncertainty naturally extends to two variables with Shannon mutual
information. Let \(s_i\) be an element of the state of \(S\), then the
Shannon mutual information \(I(S; s_i)\) is given as

\[\begin{aligned}
I(X; Y) &= \sum_{x\in S, y \in s_i} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
        &= H(S) - H(S | s_i)
\end{aligned}\]

Shannon mutual information can be interpreted as the uncertainty
reduction of \(S\) after knowing the state of \(s_i\). Consequently, it
encodes how much statistical coherence \(s_i\) and \(S\) share. Shannon
mutual information can be measured over time to encode how much
/information/ (in bits) flows from state \(s_i\) to \(S^{t}\)

\[\label{eq:flow}
\begin{aligned}
I(S^t; s_i) = H(S^t) - H(S^t | s_i).
\end{aligned}\]

Prior results showed that the nodes with the highest causal importance
are those nodes that have the highest information flow (i.e. maximize
[[#eq:flow][[eq:flow]]]) [cite:@vanElteren2022]. Intuitively, the nodes
for which the future system "remembers" information from a node in the
past, is the one that "drives" the system dynamics. Formally, these
driver nodes can be identified by computing the total information flow
between \(S^t\) and \(s_i\) can be captured with the integrated mutual
information [cite:@vanElteren2021]

\[\label{eq:imi}
\mu(s_i) = \sum_{\tau = 0}^{\infty} I(s_{i}^{t-\tau} ; S^t).\]

The driver nodes are the nodes that maximize [[#eq:imi][[eq:imi]]]. Note
that in [cite:@vanElteren2022] \(I(S  :  s_i^{t})\) was considered.
Here, information flows are computed out-of-equilibrium with symmetry
breaking. That is, the system dynamics are evolved by starting the
system at a distance from the tipping point and evolving it
out-of-equilibrium. This causes \(I(s_i^t : S)\) to not follow the data
processing inequality as information may flow back into a node. The
choice for computing \(I(s_i^t :  S)\) over \(I(s_i  : S^t)\) was done
for computational feasibility in [cite:@vanElteren2022]. Furthermore,
the data processing inequality was not violated when considered the
system without symmetry breaking. For [[#eq:flow][[eq:flow]]] the data
processing inequality is guaranteed, however it is computationally more
challenging to compute (see [[#sec:org26f073f][5]]).

** Noise matching procedure
:PROPERTIES:
:CUSTOM_ID: sec:org11ee4e3
:END:
The Boltzmann-Gibbs distribution is parameterized by noise factor
\(\beta =  \frac{1}{kT}\) where \(T\) is the temperature and \(k\) is
the Boltzmann constant. For high \(\beta\) values metastable behavior
occurs in the kinetic Ising model. The temperature was chosen such that
the statistical complexity [cite:@Lopez-Ruiz1995a] was maximized. The
statistical complexity \(C\) is computed as

\[C = \bar H(S) D(S),\]

where \(\bar H(S) = \frac{H(s)}{-\log_2(|S|)}\) is the system entropy,
and \(D(S)\) measures the distance to disequilibrium

\[D(S) = \sum_i (p(S_i) - \frac{1}{|S|})^2.\]

A typical statistical complexity curve is seen in
[[#fig:stat_compl][1]]. The noise parameter \(\beta\) is set such that
it maximizes the statistical complexity using numerical optimization
(COBYLA method in scipy's =optimize.minimize= module)
[cite:@Virtanen2020].

#+caption: (a) Statistical complexity (\(C\)), normalized system entropy
(\(H(S)\)) and disequilibrium (\(D(S)\)) as a function of the
temperature (\(T = \frac{1}{\beta}\)) for Krackhardt kite graph. The
noise parameter was set such that it maximizes the statistical
complexity (vertical black line). The values are normalized between
[0,1] for aesthetic purposes. (b) State distribution \(P(S)\) for
temperature that maximizes the statistical complexity in (a) as a
function of nodes in state +1.
[[./figures/exact_kite_dyn=ising_beta=0.5732374683235916_T=200_statistical_complexity.png]]

** Exact information flows \(I(s_i ; S^t)\)
:PROPERTIES:
:CUSTOM_ID: sec:org59af222
:END:
In order to compute \(I(s_i : S^t)\), the conditional distribution
\(p(S^t  |  s_i)\) and \(p(S^t)\) needs to be computed. For Glauber
dynamics, the system \(S\) transitions into \(S'\) by considering to
flips by randomly choosing node \(s_i\). The transition matrix
\(P(S^t |  s_i) = \textbf{P}\) can be constructed by computing each
entry \(p_{ij}\) as

\[\label{eq:glauber}
\begin{split}
p_{ij, i \neq j} &= \frac{1}{|S|} \frac{1}{ 1 + \exp (-\Delta E) }\\
p_{ii} &= 1 - \sum_{j, j \neq i} P_{ij},
\end{split}\]

where \(\Delta E =  \mathcal{H}(S_j) - \mathcal{H}(S_j)\) encodes the
energy difference of moving from \(S_i\) to \(S_j\). The state to state
transition \(\textbf{P}\) matrix will be of size \(2^{|S|} \times
2^{|S|} \times |\mathcal{A}_{s_i}|\), where \(|\mathcal{A}_{s_i}|\) is
the size of the alphabet of \(s_i\), which becomes computationally
intractable due to its exponential growth with the system size \(|S|\).
The exact information flows can then be computed by evaluating
\(p(S^t |  s_i)\) out of equilibrium by evaluating all \(S^t\) for all
possible node states \(s_i\) where \(p(S^t)\) is computed as

\[p(S^t) = \sum_{s_i} p(S^t | s_i ) p(s_i).\]

** White noise estimation procedure
:PROPERTIES:
:CUSTOM_ID: sec:orgc093508
:END:
Let \(M(S^t)\) represent the instantaneous system macrostate compute as
the system average

\[\label{}
M(S^t) = \frac{1}{|S|} \sum_i s_i.\]

The metastable behavior is characterized for the Ising model by the
system fluctuating around two stable points \(T1\) with
\(M(S) \approx 0\) and \(T2\) with \(M(S) \approx 1\) for most of the
time. By pinning intervention the node is pinned to the +0 state,
effectively biasing the macrostate \(M(S^t)\) towards \(T1\). For any
particular trajectory the fluctuations around the stable points
contributed differently for nodes depending on the nodes embedness in
the system; lower degree nodes tend to produce higher fluctuations than
higher degree nodes (see main text). We define the fluctuations as
"white noise" and characterize the white noise as function of the
pinning intervention on different nodes. The white noise is
characterized by first computing the set of windows \(W =
\{w_i  | i\in  1,  \dots  n\}\) where each window \(w_i \in  W\)
represents the duration in a trajectory that the system state stayed in
either \(T1\) or \(T2\). Next, the white noise was characterized as

\[\label{}
\eta = \frac{1}{|W|} \sqrt{ \sum_j^{|w_i|} (w_{ij} - \overline{w_i})^2},\]

where \(\overline{w_i}\) is the average of window \(w_i\) of the
instantaneous macrostate and \(w_{ij}\) is a particular instantaneous
state. The white noise was estimated for \(T1\) and \(T2\) separately
and displayed in [[#fig:kite_noise][[fig:kite_noise]]].

** Switch susceptibility as a function of degree
:PROPERTIES:
:CUSTOM_ID: sec:org009e10c
:END:
First, we investigate the susceptibility of a spin as a function of its
degree. The susceptibility of a spin switching its state is a function
both of the system temperature \(T\) and the system dynamics. The system
dynamics would contribute to the susceptibility through the underlying
network structure either directly or indirectly. The network structure
produces local correlations which affects the switch probability for a
given spin.

As an initial approximation, we consider the susceptibility of a target
spin \(s_i\) to flip from a majority state to a minority state given the
state of its neighbors where the neighbors are not connected among
themselves. Further, the assumption is that for the instantaneous update
of \(s_i\) the configuration of the neighborhood of \(s_i\) can be
considered as the outcome of a binomial trial. Let, \(N\) be a random
variable with state space \(\{0,  1\}^{|N|}\), and let \(n_j \in N\)
represent a neighbor of \(s_i\). We assume that all neighbors of \(s_i\)
are i.i.d. distributed given the instantaneous system magnetization

\[M(S^t) = \frac{1}{|S^t|} \sum_i s_i^t.\]

Let the minority state be 1 and the majority state be 0, the expectation
of \(s_i\) flipping from the majority state to the minority state is
given as:

\[\begin{aligned}
\label{majority_flip}
    \begin{aligned}
        E[ p(s_i = 1 | N ) ]_{p(N)} &= \sum_{N_i \in N} p(N_i) p(s_i = 1 | N_i)\\
            &= \sum_{N_i \in  N} \prod_j^{|N_i|} p(n_j) p(s_i  = 1 |N_i)\\
            &=  \sum_{N_i \in N}  {n\choose k} f^k  (1  -
            f)^{n-k}  p(s_i  = 1 | f), \\
    \end{aligned}\end{aligned}\]

where \(f\) is the fraction of nodes in the majority states, \(n\) is
the number of neighbors, \(k\) is the number of nodes in state 0. In
figure [[#fig:maj_flip][2]]. this is computed as a function of the
degree of spin \(s_i\). As the degree increases, the susceptibility for
a spin decreases relatively to the same spin with a lower degree. This
implies that the susceptibility of change to random fluctuations are
more likely to occur in nodes with less external constraints as measured
by degree.

* Information flows with SIS dynamics
:PROPERTIES:
:CUSTOM_ID: information-flows-with-sis-dynamics
:END:

[[./figures/sis_kite_graph.pdf]]

* Flip probability per degree
:PROPERTIES:
:CUSTOM_ID: sec:deg_flip
:END:
In [[#fig:maj_flip][2]] the tendency for a node to flip from the
majority to the minority state is computed as function of fraction of
nodes possessing the majority states +1 in the system, denoted as \(N\).
Two things are observed. First, nodes with lower degree are more
susceptible to noise than nodes with higher degree. For a given system
stability, nodes with lower degree tend to have a higher tendency to
flip. This is true for all distances of the system to the tipping point.
In contrast, the higher the degree of the node, the closer the system
has to be to a tipping point for the node to change its state. This can
be explained by the fact that lower degree nodes, have fewer constraints
compared to nodes with higher degree nodes. For Ising spin kinetics, the
nodes with higher degree tend to be more "frozen" in their node dynamics
than nodes with lower degree. Second, in order for a node to flip with
probability with similar mass, i.e. (\(E[p(s_i) |  N] = 0.2\)) a node
with higher degree needs to be closer to the tipping point than nodes
with lower degree. In fact, the order of susceptibility is correlated
with the degree; the susceptibility decreases with increasing degree and
fixed fraction of nodes in state 1.

#+caption: Susceptibility of a node with degree \(k\) switching from the
minority state  0 to the majority  state 1 as a  function of
the neighborhood  entropy for \(\beta =  0.5\). The neighborhood
entropy encodes how stable the  environment of a spin is. As
the system approaches the tipping point, the propensity of a
node to flip from to the minority state increases faster for
low degree nodes  than for high degree  nodes. Higher degree
nodes require more change in their local environment to flip
to     the    majority     state.     See    for     details
[[#sec:org009e10c][8.7]].
[[./figures/fig_majority_flip.pdf]]
